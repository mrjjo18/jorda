{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **계산항 정리** \n",
    "\n",
    "$S_t =$ State의 집합 {$(1,1), (1,2), (1,3)$}\n",
    "\n",
    "$A_t =$ Action의 집합\n",
    "\n",
    "$r^a_s = E[r_{t+1} | S_t = s, A_t = a]$\n",
    "\n",
    "$G_t = r_{t+1} + r_{t+2} + r_{t+3} ... r_{t+n}$\n",
    "\n",
    "$P^a_{ss'} = P[S_{t+1} = s' | S_t = s, A_t = a]$\n",
    "\n",
    "$\\gamma$ = Discount Factor\n",
    "\n",
    "$\\pi(a|s) = P[A_t = a | S_t = s]$\n",
    "\n",
    "\n",
    "\n",
    "### **벨만 기대 방정식(해당 state에서 얻을 수 있는 가치의 합계)**\n",
    "$$v_\\pi(s) = E(G_t)$$  \n",
    "$$v_\\pi(s) = \\sum_{a{\\in}A}\\pi(a|s)q_\\pi(s,a)$$  \n",
    "$$q_\\pi(s,a) = r^a_s + \\gamma\\sum_{s'{\\in}S}P^a_{ss'}\\sum_{a'{\\in}A}\\pi(a'|s')q_{\\pi}(s',a')$$\n",
    "$$v_\\pi(s) = \\sum_{a{\\in}A}\\pi(a|s)\\biggl(r^a_s + \\gamma\\sum_{s'{\\in}S}P^a_{ss'}v_{\\pi}(s')\\biggl)$$ \n",
    "\n",
    "#### -> 벨만 기대방정식(상태가치함수)을 크게 두 항으로 나누면  1) 특정 state에서 특정 action을 할 확률과, 2) 그 state 에서 action을 했을 때 얻을 수 있는 보상(현재 + 미래)으로 이루어지며, 이들의 합으로 상태가치함수를 구할 수 있음\n",
    "\n",
    "\n",
    "**MDP를 알 때**\n",
    "\n",
    "1. 보상함수  -> state에 따른 penalty와 reward를 알고 있다.(-1, 1 등)\n",
    "\n",
    "2. 상태전이확률 -> s에서 a를 했을 때 s'으로 전이할 확률을 알고 있다. (1로 고정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최고의 정책 찾기**\n",
    "\n",
    "\n",
    "- **정책 평가**와 **정책 개선**을 번갈아 수행하여 정책이 수렴할 때까지 **반복**한다.(정책 수렴 = 최적의 정책을 찾을 때 까지 -> $\\pi(a|s)$업데이트)\n",
    "- 위 과정의 반복을 통해 기존 정책 $\\pi$에 비해 새로운 정책 $\\pi'$가 개선되는 것이 핵심(교재엔 없지만 방법론을 증명하는 수많은 정리와 증명이 존재함)\n",
    "- 특정 state마다 greedy한 선택을 하면 결국 최종적으로 얻는 가치함수도 클 것이다.  \n",
    "($s_t$ 주변에서 가치함수가 가장 높은 state로 가는 선택을 한다.  -> $s_t+1$에서도 그렇게 한다. -> 결국 최적의 가치를 주는 state로 움직일 것이다.)\n",
    "- 벨만 '기대' 방정식 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체 선언\n",
    "        self.env = env\n",
    "        # 가치함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # 상 하 좌 우 동일한 확률로 정책 초기화\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                            for _ in range(env.height)]\n",
    "        # 마침 상태의 설정\n",
    "        self.policy_table[2][2] = []\n",
    "        # 할인율\n",
    "        self.discount_factor = 0.9\n",
    "        \n",
    "        \n",
    "    # 벨만 기대 방정식을 통해 다음 상태 가치 함수를 계산하는 정책 평가\n",
    "    def policy_evaluation(self):\n",
    "        # 다음 가치함수 초기화\n",
    "\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                           for _ in range(self.env.height)]\n",
    "        \n",
    "        \n",
    "        # 모든 상태에 대해서 벨만 기대방정식을 계산 - 상태별로 좌우상하에 대한 벨만방정식을 계산하여 합산한 값이 칸칸에 있는 값\n",
    "        print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            # 벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = value\n",
    "            print(\"벨만기대방정식을 통해 state = {}의 상태가치함수 계산(열,행 순서): \".format(state), value)\n",
    "        \n",
    "        \n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
    "    # 해당 state에서 한 스텝을 그리디하게 움직이고 이후에는 기존 정책을 따르는 것이 좋다는 말은 결국 모든 state에서 그리디한 것이 좋다는 의미\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        \n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            \n",
    "            value_list = []\n",
    "            # 반환할 정책 초기화\n",
    "            result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "            # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                #현재 상태에서 가능한 행동에 대한 r+discounted_future_value\n",
    "                value = reward + self.discount_factor * next_value\n",
    "                value_list.append(value)\n",
    "                \n",
    "            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전(np.amx(value_list))\n",
    "            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "            max_idx_list = max_idx_list.flatten().tolist()\n",
    "            prob = 1 / len(max_idx_list)\n",
    "\n",
    "            for idx in max_idx_list:\n",
    "                result[idx] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "            print(\"state = \"+\"[\"+str(state[0])+\", \"+str(state[1])+\"]\"+\"의 action policy(열,행 / 좌우상하 순서): \" + str(next_policy[state[0]][state[1]]))\n",
    "            #좌우상하 순서고, 열/행 순서임\n",
    "            \n",
    "\n",
    "        self.policy_table = next_policy\n",
    "#        print(self.policy_table)\n",
    "\n",
    "    # 특정 상태에서 정책에 따라 무작위로 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        policy = self.get_policy(state)\n",
    "        policy = np.array(policy)\n",
    "        #choice 이후 4는 행동의 개수(좌우상하), 1은 몇 개의 행동을 샘플링 할지, 세 번째는 랜덤이지만 각 행동을 얼마의 확률에 기반해서 샘플링할 것인가? 이걸 넣으면 행동이 정해짐\n",
    "        return np.random.choice(4, 1, p=policy)[0]\n",
    "        \n",
    "        \n",
    "    # 상태에 따른 정책 반환\n",
    "    def get_policy(self, state):\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "        \n",
    "    # 가치 함수의 값을 반환\n",
    "    def get_value(self, state):\n",
    "        return self.value_table[state[0]][state[1]]\n",
    "        print(self.value_table[state[0]][state[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그리드 월드 칸칸의 값 = 해당 state 이후의 state에서 액션 좌우상하를 했을 때 얻을 수 있는 가치의 합계\n",
    "# e.g. k = 1일때, (3,1)은 v = -0.25인데, 이는 pi(a|s) * (t+1시점의 reward + 감가율 * 미래가치 합계로 계산됨)\n",
    "# 상태전환확률을 1로 가정하기 때문에, 좌로 이동시 -> 0.25 * (-1 + (0.9 * 0)) = -0.25 \n",
    "# 상태전환확률을 1로 가정하기 때문에, 우로 이동시 -> 0.25 * (0 + (0.9 * 0)) = 0\n",
    "# 상태전환확률을 1로 가정하기 때문에, 상으로 이동시 -> 0.25 * (0 + (0.9 * 0)) = 0\n",
    "# 상태전환확률을 1로 가정하기 때문에, 하로 이동시 -> 0.25 * (0 + (0.9 * 0)) = 0\n",
    "# -> SUM(상하좌우v) = 0.25 -> 해당 state의 k=1 상태가치함수의 값은 -0.25\n",
    "\n",
    "# e.g. k = 2일때, (3,1)은 v = 0.23인데, 이는 pi(a|s) * (t+1시점의 reward * 감가율 * 미래가치 합계로 계산됨)\n",
    "# 상태전환확률을 1로 가정하기 때문에, 좌로 이동시 -> 0.25 * (-1 + (0.9 * 0.25)) = -0.19375인데, 리워드가 -1이기 때문에 이 액션은 배제함 pi(a|s) = 0\n",
    "# 상태전환확률을 1로 가정하기 때문에, 우로 이동시 -> 0.25 * (0 + (0.9 * 0)) = 0  pi(a|s) = 0\n",
    "# 상태전환확률을 1로 가정하기 때문에, 상으로 이동시 -> 0.25 * (0 + (0.9 * 0)) = 0 pi(a|s) = 0\n",
    "# 상태전환확률을 1로 가정하기 때문에, 하로 이동시 -> 1 * (0 + (0.9 * 0.25)) = 0.225 \n",
    "# -> SUM(상하좌우v) = 0.225 -> 해당 state의 k=2 상태가치함수의 값은 0.225\n",
    "\n",
    "# e.g. k = 3일때, (3,1)은 v = 0.9인데, 이는 pi(a|s) * (t+1시점의 reward * 감가율 * 미래가치 합계로 계산됨)\n",
    "# 상태전환확률을 1로 가정하기 때문에, 좌로 이동시 -> 0.0 * (1 + (0.9 * 1)) = 0\n",
    "# 상태전환확률을 1로 가정하기 때문에, 우로 이동시 -> 0.0 * (0 + (0.9 * 0.06)) = 0\n",
    "# 상태전환확률을 1로 가정하기 때문에, 상으로 이동시 -> 0.0 * (0 + (0.9 * -0.14)) = 0 (위 셀의 미래가치 합계가 계산되었기 때문에)\n",
    "# 상태전환확률을 1로 가정하기 때문에, 하로 이동시 -> 1 * (0 + (0.9 * 1)) = 0.9\n",
    "# -> SUM(상하좌우v) = 0.9 -> 해당 state의 k = 3 상태가치함수의 값은 0.9 -> 이런 식으로 무한 반복하면 state별 상태가치 함수를 수렴시킬 수 있고 결국에 최적의 정책을 찾아낼 수 있음.\n",
    "\n",
    "# 모두 훈련하고 난 후 state 0, 0 기준으로 벨만기대방정식 계산값은 0.59인데\n",
    "# 이는 0,0기준 오른쪽으로 이동했을 때의 기댓값 0.5 * (0 + (0.9 * 0.66)) + 아래로 이동했을 때의 기댓값 0.5 * (0 + (0.9 * 0.66)) 을 더해서 0.594가 나오는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "벨만기대방정식을 통해 state = [0, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 2]의 상태가치함수 계산(열,행 순서):  -0.25\n",
      "벨만기대방정식을 통해 state = [0, 3]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 1]의 상태가치함수 계산(열,행 순서):  -0.5\n",
      "벨만기대방정식을 통해 state = [1, 2]의 상태가치함수 계산(열,행 순서):  0.25\n",
      "벨만기대방정식을 통해 state = [1, 3]의 상태가치함수 계산(열,행 순서):  -0.25\n",
      "벨만기대방정식을 통해 state = [1, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [2, 0]의 상태가치함수 계산(열,행 순서):  -0.25\n",
      "벨만기대방정식을 통해 state = [2, 1]의 상태가치함수 계산(열,행 순서):  0.25\n",
      "벨만기대방정식을 통해 state = [2, 3]의 상태가치함수 계산(열,행 순서):  0.25\n",
      "벨만기대방정식을 통해 state = [2, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [3, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [3, 1]의 상태가치함수 계산(열,행 순서):  -0.25\n",
      "벨만기대방정식을 통해 state = [3, 2]의 상태가치함수 계산(열,행 순서):  0.25\n",
      "벨만기대방정식을 통해 state = [3, 3]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [3, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 2]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 3]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "state = [0, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [0, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [0, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.5, 0.5]\n",
      "state = [0, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [0, 4]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [1, 0]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 4]의 action policy(열,행 / 좌우상하 순서): [0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333]\n",
      "state = [2, 0]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.5, 0.0, 0.0]\n",
      "state = [2, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [2, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [2, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [3, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [3, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [3, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [3, 4]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [4, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [4, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "state = [4, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [4, 3]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [4, 4]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "벨만기대방정식을 통해 state = [0, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 2]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 3]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [1, 3]의 상태가치함수 계산(열,행 순서):  0.225\n",
      "벨만기대방정식을 통해 state = [1, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [2, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [2, 1]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 3]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 4]의 상태가치함수 계산(열,행 순서):  0.225\n",
      "벨만기대방정식을 통해 state = [3, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [3, 1]의 상태가치함수 계산(열,행 순서):  0.225\n",
      "벨만기대방정식을 통해 state = [3, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [3, 3]의 상태가치함수 계산(열,행 순서):  0.225\n",
      "벨만기대방정식을 통해 state = [3, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 2]의 상태가치함수 계산(열,행 순서):  0.225\n",
      "벨만기대방정식을 통해 state = [4, 3]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "state = [0, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [0, 1]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [0, 2]의 action policy(열,행 / 좌우상하 순서): [0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333]\n",
      "state = [0, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [0, 4]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [1, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [1, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [2, 0]의 action policy(열,행 / 좌우상하 순서): [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0]\n",
      "state = [2, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [2, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [2, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [3, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [3, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [3, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [4, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [4, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 4]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "벨만기대방정식을 통해 state = [0, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 2]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 3]의 상태가치함수 계산(열,행 순서):  0.2025\n",
      "벨만기대방정식을 통해 state = [0, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [1, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [1, 4]의 상태가치함수 계산(열,행 순서):  0.2025\n",
      "벨만기대방정식을 통해 state = [2, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [2, 1]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 3]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 4]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 0]의 상태가치함수 계산(열,행 순서):  0.2025\n",
      "벨만기대방정식을 통해 state = [3, 1]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [3, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 4]의 상태가치함수 계산(열,행 순서):  0.2025\n",
      "벨만기대방정식을 통해 state = [4, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [4, 1]의 상태가치함수 계산(열,행 순서):  0.2025\n",
      "벨만기대방정식을 통해 state = [4, 2]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [4, 3]의 상태가치함수 계산(열,행 순서):  0.2025\n",
      "벨만기대방정식을 통해 state = [4, 4]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "state = [0, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [0, 1]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [0, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [0, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [1, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [1, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [2, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [2, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [2, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [2, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [3, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [3, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [3, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 0]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [4, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "벨만기대방정식을 통해 state = [0, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 2]의 상태가치함수 계산(열,행 순서):  0.18225000000000002\n",
      "벨만기대방정식을 통해 state = [0, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [0, 4]의 상태가치함수 계산(열,행 순서):  0.18225000000000002\n",
      "벨만기대방정식을 통해 state = [1, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [1, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [1, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [2, 0]의 상태가치함수 계산(열,행 순서):  0.18225000000000002\n",
      "벨만기대방정식을 통해 state = [2, 1]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 3]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 4]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 0]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [3, 1]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [3, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 0]의 상태가치함수 계산(열,행 순서):  0.18225000000000002\n",
      "벨만기대방정식을 통해 state = [4, 1]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 2]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [4, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 4]의 상태가치함수 계산(열,행 순서):  0.18225000000000002\n",
      "state = [0, 0]의 action policy(열,행 / 좌우상하 순서): [0.25, 0.25, 0.25, 0.25]\n",
      "state = [0, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [0, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [1, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [2, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [2, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [2, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [2, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [3, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [3, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [3, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 0]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [4, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "벨만기대방정식을 통해 state = [0, 0]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [0, 1]의 상태가치함수 계산(열,행 순서):  0.16402500000000003\n",
      "벨만기대방정식을 통해 state = [0, 2]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [0, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [0, 4]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [1, 0]의 상태가치함수 계산(열,행 순서):  0.16402500000000003\n",
      "벨만기대방정식을 통해 state = [1, 1]의 상태가치함수 계산(열,행 순서):  0.0\n",
      "벨만기대방정식을 통해 state = [1, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [1, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [1, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [2, 0]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [2, 1]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 3]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 4]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 0]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [3, 1]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [3, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 0]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [4, 1]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 2]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [4, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 4]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "state = [0, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.0, 0.5]\n",
      "state = [0, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [0, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [1, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [2, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [2, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [2, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [2, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [3, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [3, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [3, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 0]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [4, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "벨만기대방정식을 통해 state = [0, 0]의 상태가치함수 계산(열,행 순서):  0.14762250000000005\n",
      "벨만기대방정식을 통해 state = [0, 1]의 상태가치함수 계산(열,행 순서):  0.6561000000000001\n",
      "벨만기대방정식을 통해 state = [0, 2]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [0, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [0, 4]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [1, 0]의 상태가치함수 계산(열,행 순서):  0.6561000000000001\n",
      "벨만기대방정식을 통해 state = [1, 1]의 상태가치함수 계산(열,행 순서):  0.14762250000000005\n",
      "벨만기대방정식을 통해 state = [1, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [1, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [1, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [2, 0]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [2, 1]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 3]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 4]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 0]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [3, 1]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [3, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 0]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [4, 1]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 2]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [4, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 4]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "state = [0, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.0, 0.5]\n",
      "state = [0, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [0, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [1, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [2, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [2, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [2, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [2, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [3, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [3, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [3, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 0]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [4, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "벨만기대방정식을 통해 state = [0, 0]의 상태가치함수 계산(열,행 순서):  0.5904900000000002\n",
      "벨만기대방정식을 통해 state = [0, 1]의 상태가치함수 계산(열,행 순서):  0.6561000000000001\n",
      "벨만기대방정식을 통해 state = [0, 2]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [0, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [0, 4]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [1, 0]의 상태가치함수 계산(열,행 순서):  0.6561000000000001\n",
      "벨만기대방정식을 통해 state = [1, 1]의 상태가치함수 계산(열,행 순서):  0.5904900000000002\n",
      "벨만기대방정식을 통해 state = [1, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [1, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [1, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [2, 0]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [2, 1]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 3]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [2, 4]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 0]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [3, 1]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 2]의 상태가치함수 계산(열,행 순서):  1.0\n",
      "벨만기대방정식을 통해 state = [3, 3]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [3, 4]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 0]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "벨만기대방정식을 통해 state = [4, 1]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 2]의 상태가치함수 계산(열,행 순서):  0.9\n",
      "벨만기대방정식을 통해 state = [4, 3]의 상태가치함수 계산(열,행 순서):  0.81\n",
      "벨만기대방정식을 통해 state = [4, 4]의 상태가치함수 계산(열,행 순서):  0.7290000000000001\n",
      "state = [0, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.0, 0.5]\n",
      "state = [0, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [0, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [0, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [1, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [1, 2]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [1, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.5, 0.5, 0.0]\n",
      "state = [2, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 1.0, 0.0, 0.0]\n",
      "state = [2, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [2, 3]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [2, 4]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 1.0, 0.0]\n",
      "state = [3, 0]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 1]의 action policy(열,행 / 좌우상하 순서): [0.0, 0.0, 0.0, 1.0]\n",
      "state = [3, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [3, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [3, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 0]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 1]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.0, 0.5]\n",
      "state = [4, 2]의 action policy(열,행 / 좌우상하 순서): [1.0, 0.0, 0.0, 0.0]\n",
      "state = [4, 3]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n",
      "state = [4, 4]의 action policy(열,행 / 좌우상하 순서): [0.5, 0.0, 0.5, 0.0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    policy_iteration = PolicyIteration(env)\n",
    "    grid_world = GraphicDisplay(policy_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
